import tensorflow as tf
from tensorflow.keras.applications import ResNet50V2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
import numpy as np

# --- Configuration and Hyperparameters ---
IMAGE_SIZE = (224, 224)
NUM_CLASSES = 10  # Example: 10 categories
BATCH_SIZE = 32
LEARNING_RATE = 1e-3
EPOCHS_CLASSIFIER = 5
EPOCHS_FINE_TUNE = 5
FINE_TUNE_LAYER_COUNT = 50 # Number of top convolutional layers to unfreeze
# ----------------------------------------

# --- 1. Simulate Data Loading (Replace with actual data loading) ---
# In a real scenario, use tf.keras.preprocessing.image_dataset_from_directory or similar
print("Simulating data loading...")
def create_dummy_data(count, size, num_classes):
    X = np.random.rand(count, *size, 3).astype('float32')
    Y = tf.keras.utils.to_categorical(np.random.randint(0, num_classes, count), num_classes=num_classes)
    return X, Y

X_train, Y_train = create_dummy_data(1000, IMAGE_SIZE, NUM_CLASSES)
X_val, Y_val = create_dummy_data(200, IMAGE_SIZE, NUM_CLASSES)
print(f"Train samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}")

# --- a. Load in a pre-trained CNN model (Feature Extractor) ---
print("\na. Loading pre-trained ResNet50V2 model...")
base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))

# --- b. Freeze parameters in model's lower convolutional layers ---
print("b. Freezing base model layers...")
base_model.trainable = False

# --- c. Add custom classifier with several layers of trainable parameters ---
print("c. Building custom classifier on top...")
x = base_model.output
x = GlobalAveragePooling2D()(x) # Reduce spatial dimensions
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(NUM_CLASSES, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Print summary of the initial setup (only classifier layers are trainable)
print("\nModel Summary (Stage 1: Frozen Base Model):")
# model.summary() # Uncomment for full summary

# --- d. Train classifier layers on training data available for task ---
print(f"\nd. Training custom classifier for {EPOCHS_CLASSIFIER} epochs...")
history_classifier = model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    epochs=EPOCHS_CLASSIFIER,
    batch_size=BATCH_SIZE,
    verbose=1
)

# --- e. Fine-tune hyperparameters and unfreeze more layers as needed ---
print("\ne. Starting Fine-Tuning Stage (Unfreezing top layers)...")

# Unfreeze the base model
base_model.trainable = True

# Freeze all layers except the top 'FINE_TUNE_LAYER_COUNT' layers
for layer in base_model.layers[:-FINE_TUNE_LAYER_COUNT]:
    layer.trainable = False
# Note: Batch Normalization layers should often remain frozen

# Re-compile the model with a lower learning rate for fine-tuning
FINE_TUNE_LR = LEARNING_RATE / 10
model.compile(optimizer=Adam(learning_rate=FINE_TUNE_LR),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print(f"Re-compiling with LR={FINE_TUNE_LR} and unfreezing top {FINE_TUNE_LAYER_COUNT} layers of ResNet50V2.")
# print("\nModel Summary (Stage 2: Partially Unfrozen Base Model):")
# model.summary() # Uncomment for full summary

# Continue training for the fine-tuning stage
history_fine_tune = model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    epochs=EPOCHS_CLASSIFIER + EPOCHS_FINE_TUNE, # Train for total epochs
    initial_epoch=history_classifier.epoch[-1] + 1,
    batch_size=BATCH_SIZE,
    verbose=1
)

print("\n--- Transfer Learning Complete ---")
loss, acc = model.evaluate(X_val, Y_val, verbose=0)
print(f"Final Validation Accuracy: {acc*100:.2f}%")
